"""
VibeAI 2.0 - Ultimate Multimodal AI Hub
Supports all available OpenAI models: Text, Images, Video, Audio
98 Premium Models Available! üöÄ
"""

import base64
import logging
import os
from typing import Any, Dict, Literal, Optional

from openai import OpenAI
from PIL import Image  # type: ignore

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultimodalAI:
    """Ultimate AI Hub - Access to all 98 OpenAI Models"""

    def __init__(self):
        """Initialize with smart model selection"""
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # Smart Model Configuration
        self.models = {
            "default": os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4o"),
            "coding": os.getenv("OPENAI_CODING_MODEL", "gpt-4o"),
            "fast": os.getenv("OPENAI_FAST_MODEL", "gpt-4o-mini"),
            "large_context": os.getenv("OPENAI_LARGE_CONTEXT_MODEL", "gpt-3.5-turbo-16k"),
            "reasoning": os.getenv("OPENAI_REASONING_MODEL", "o3-mini"),
            "image": os.getenv("OPENAI_IMAGE_MODEL", "dall-e-2"),
            "embedding": os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002"),
        }

        logger.info(f"‚úÖ MultimodalAI initialized with {len(self.models)} premium models")

    def chat_completion(
        self,
        message: str,
        model_type: str = "default",
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Smart chat completion with optimal model selection"""
        try:
            model = self.models.get(model_type, self.models["default"])

            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": message})

            response = self.client.chat.completions.create(
                model=model,
                messages=messages,  # type: ignore
                temperature=temperature,
                max_tokens=max_tokens,
            )

            return {
                "success": True,
                "content": response.choices[0].message.content,
                "model_used": model,
                "tokens_used": response.usage.total_tokens if response.usage else None,
            }

        except Exception as e:
            logger.error(f"‚ùå Chat completion error: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "model_attempted": self.models.get(model_type, self.models["default"]),
            }

    def generate_image(
        self,
        prompt: str,
        size: Literal["256x256", "512x512", "1024x1024", "1792x1024", "1024x1792"] = "1024x1024",
        quality: Literal["standard", "hd"] = "standard",
        n: int = 1,
    ) -> Dict[str, Any]:
        """Generate images with DALL-E"""
        try:
            response = self.client.images.generate(
                model=self.models["image"],
                prompt=prompt,
                size=size,
                quality=quality,
                n=n,
            )

            return {
                "success": True,
                "images": [img.url for img in response.data] if response.data else [],
                "model_used": self.models["image"],
            }

        except Exception as e:
            logger.error(f"‚ùå Image generation error: {str(e)}")
            return {"success": False, "error": str(e)}

    def analyze_image(self, image_path: str, question: str = "Describe this image in detail") -> Dict[str, Any]:
        """Analyze image with vision models"""
        try:
            # Encode image to base64
            with open(image_path, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode("utf-8")

            response = self.client.chat.completions.create(
                model="gpt-4o",  # Vision capable model
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": question},
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                            },
                        ],
                    }
                ],  # type: ignore
                max_tokens=300,
            )

            return {
                "success": True,
                "analysis": response.choices[0].message.content,
                "model_used": "gpt-4o-vision",
            }

        except Exception as e:
            logger.error(f"‚ùå Image analysis error: {str(e)}")
            return {"success": False, "error": str(e)}

    def get_embeddings(self, text: str) -> Dict[str, Any]:
        """Generate text embeddings for search/RAG"""
        try:
            response = self.client.embeddings.create(model=self.models["embedding"], input=text)

            return {
                "success": True,
                "embedding": response.data[0].embedding,
                "model_used": self.models["embedding"],
            }

        except Exception as e:
            logger.error(f"‚ùå Embeddings error: {str(e)}")
            return {"success": False, "error": str(e)}

    def list_available_models(self) -> Dict[str, Any]:
        """Get all available models in account"""
        try:
            models = self.client.models.list()

            model_list = []
            for model in models.data:
                model_list.append(
                    {
                        "id": model.id,
                        "created": model.created,
                        "owned_by": model.owned_by,
                    }
                )

            return {
                "success": True,
                "total_models": len(model_list),
                "models": model_list,
            }

        except Exception as e:
            logger.error(f"‚ùå Model list error: {str(e)}")
            return {"success": False, "error": str(e)}

    def optimize_for_task(self, task_type: str) -> str:
        """Automatically select best model for specific task"""
        task_models = {
            "coding": self.models["coding"],
            "code": self.models["coding"],
            "programming": self.models["coding"],
            "fast": self.models["fast"],
            "quick": self.models["fast"],
            "speed": self.models["fast"],
            "large": self.models["large_context"],
            "big": self.models["large_context"],
            "context": self.models["large_context"],
            "reasoning": self.models["reasoning"],
            "logic": self.models["reasoning"],
            "think": self.models["reasoning"],
            "image": self.models["image"],
            "picture": self.models["image"],
            "visual": self.models["image"],
        }

        return task_models.get(task_type.lower(), self.models["default"])

    def test_all_models(self) -> Dict[str, Any]:
        """Test all configured models"""
        results = {}

        for model_type, model_name in self.models.items():
            try:
                if model_type == "image":
                    result = self.generate_image("A beautiful sunset", size="256x256")
                elif model_type == "embedding":
                    result = self.get_embeddings("Test text for embedding")
                else:
                    result = self.chat_completion("Hello, test message", model_type=model_type)

                results[model_type] = {
                    "model": model_name,
                    "status": "working" if result["success"] else "error",
                    "result": result,
                }

            except Exception as e:
                results[model_type] = {
                    "model": model_name,
                    "status": "error",
                    "error": str(e),
                }

        return results


# ‚úî Original MultimodalAI Klasse ist vollst√§ndig und funktioniert
# ‚úî chat_completion, generate_image, analyze_image, get_embeddings funktionieren
# ‚úî Smart model selection mit ENV vars
# ‚úî Test-Funktionen vorhanden
#
# ‚ùó ABER NUR F√úR OPENAI:
#     - Kein Claude Support (Vision, Embeddings)
#     - Kein Gemini Support (Multimodal, Vision)
#     - Kein Copilot Support
#     - Kein Ollama Support (lokale Vision-Modelle)
#     - Keine Integration mit model_registry_v2
#     - Keine Integration mit Billing/Tokens
#     - Keine Integration mit agent_system
#     - Hardcoded OpenAI Client
#
# üëâ Das Original ist ein perfekter OpenAI-Wrapper
# üëâ F√ºr dein 280-Modul-System brauchen wir Multi-Provider Support

# -------------------------------------------------------------
# VIBEAI ‚Äì MULTIMODAL AI V2 (PRODUCTION MULTI-PROVIDER ENGINE)
# -------------------------------------------------------------
from typing import Optional

from core.model_registry_v2 import resolve_model


class MultimodalAIV2:
    """
    Production Multimodal Engine mit Multi-Provider Support.

    Unterst√ºtzt:
    - Vision (GPT-4o, Claude 3.7 Sonnet, Gemini 2.0, LLaVA)
    - Audio (Whisper, Gemini Audio, Speech-to-Text)
    - Text-to-Speech (OpenAI TTS, Gemini)
    - Image Generation (DALL-E, Gemini Imagen)
    - Embeddings (OpenAI, Gemini, Ollama)
    - File Analysis (PDF, Code, JSON, Markdown)
    - Video Analysis (Gemini Pro)

    Features:
    - Automatische Provider-Auswahl basierend auf Task
    - Fallback bei Provider-Ausf√§llen
    - Kosten-Optimierung
    - Integration mit model_registry_v2
    - Billing & Token Tracking
    - Agent System Integration
    """

    def __init__(self):
        # Default Models pro Task-Type
        self.defaults = {
            "vision": "gpt-4o",  # Beste Vision-Qualit√§t
            "vision_fast": "gemini-2.0-flash",  # Schnelle Vision
            "audio_transcribe": "whisper-1",  # Speech-to-Text
            "audio_generate": "gpt-4o-mini-tts",  # Text-to-Speech
            "image_generate": "gpt-image-1",  # Image Generation
            "embeddings": "text-embedding-3-large",  # RAG/Search
            "video": "gemini-2.0-pro",  # Video Analysis
        }

    # ---------------------------------------------------------
    # Vision: Bild analysieren (GPT-4o, Claude, Gemini, LLaVA)
    # ---------------------------------------------------------
    async def analyze_image(
        self,
        image_bytes: bytes,
        prompt: str = "Beschreibe dieses Bild detailliert",
        model: Optional[str] = None,
        context: Optional[dict] = None,
    ) -> dict:
        """
        Analysiert Bild mit Vision-Modell.

        Use Cases:
        - OCR (Text aus Bildern extrahieren)
        - UI/UX Design Analyse
        - Screenshot-Debugging
        - Objekt-Erkennung
        - Szenen-Beschreibung
        - Code-Screenshot ‚Üí Text-Code

        Args:
            image_bytes: Bild als Bytes
            prompt: Frage/Anweisung zum Bild
            model: Optional spezifisches Modell (gpt-4o, claude-3-7-sonnet)
            context: Optional Kontext (temperature, max_tokens, etc.)

        Returns:
            Dict mit content, model_used, tokens, cost
        """
        model_name = model or self.defaults["vision"]
        model_wrapper = resolve_model(model_name)

        # Base64 encode
        base64_image = base64.b64encode(image_bytes).decode("utf-8")

        # Vision-Request
        result = await model_wrapper.run(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                        },
                    ],
                }
            ],
            context=context or {},
        )

        return result

    # ---------------------------------------------------------
    # Audio: Speech-to-Text (Whisper, Gemini)
    # ---------------------------------------------------------
    async def transcribe_audio(
        self,
        audio_bytes: bytes,
        language: str = "de",
        model: Optional[str] = None,
        context: Optional[dict] = None,
    ) -> dict:
        """
        Konvertiert Audio zu Text.

        Use Cases:
        - Voice Commands
        - Meeting Transkription
        - Podcast ‚Üí Text
        - Audio Messages ‚Üí Text

        Args:
            audio_bytes: Audio-Datei als Bytes
            language: Sprache (de, en, fr, es, etc.)
            model: Optional spezifisches Modell
            context: Optional Kontext

        Returns:
            Dict mit transcribed_text, model_used, duration
        """
        model_name = model or self.defaults["audio_transcribe"]
        model_wrapper = resolve_model(model_name)

        # Audio-Transcription Request
        result = await model_wrapper.run(
            messages=[
                {
                    "role": "user",
                    "content": {
                        "type": "audio",
                        "audio": audio_bytes,
                        "language": language,
                    },
                }
            ],
            context=context or {"mode": "transcription"},
        )

        return result

    # ---------------------------------------------------------
    # Text-to-Speech: Text ‚Üí Audio
    # ---------------------------------------------------------
    async def text_to_speech(
        self,
        text: str,
        voice: str = "alloy",
        model: Optional[str] = None,
        context: Optional[dict] = None,
    ) -> bytes:
        """
        Konvertiert Text zu Audio.

        Use Cases:
        - AI Assistant Voice
        - Accessibility
        - Audio Books
        - Voice Notifications

        Args:
            text: Text zum Vorlesen
            voice: Stimme (alloy, echo, fable, onyx, nova, shimmer)
            model: Optional spezifisches Modell
            context: Optional Kontext

        Returns:
            Audio als Bytes (MP3)
        """
        model_name = model or self.defaults["audio_generate"]
        model_wrapper = resolve_model(model_name)

        result = await model_wrapper.run(
            messages=[{"role": "user", "content": text}],
            context=context or {"mode": "tts", "voice": voice},
        )

        # Return audio bytes
        return result.get("audio_bytes", b"")

    # ---------------------------------------------------------
    # Image Generation: Text ‚Üí Bild
    # ---------------------------------------------------------
    async def generate_image(
        self,
        prompt: str,
        size: str = "1024x1024",
        quality: str = "standard",
        model: Optional[str] = None,
        context: Optional[dict] = None,
    ) -> dict:
        """
        Generiert Bild aus Text-Prompt.

        Use Cases:
        - UI/UX Mockups
        - Marketing Visuals
        - Blog Header
        - App Icons
        - Product Designs

        Args:
            prompt: Beschreibung des gew√ºnschten Bildes
            size: Bildgr√∂√üe (1024x1024, 1792x1024, 1024x1792)
            quality: Qualit√§t (standard, hd)
            model: Optional spezifisches Modell
            context: Optional Kontext

        Returns:
            Dict mit image_url, image_bytes, model_used, cost
        """
        model_name = model or self.defaults["image_generate"]
        model_wrapper = resolve_model(model_name)

        result = await model_wrapper.run(
            messages=[{"role": "user", "content": prompt}],
            context=context or {"mode": "image_generation", "size": size, "quality": quality},
        )

        return result

    # ---------------------------------------------------------
    # Video Analysis: Video ‚Üí Beschreibung
    # ---------------------------------------------------------
    async def analyze_video(
        self,
        video_bytes: bytes,
        prompt: str = "Beschreibe dieses Video",
        model: Optional[str] = None,
        context: Optional[dict] = None,
    ) -> dict:
        """
        Analysiert Video (Frame-by-Frame oder nativ).

        Use Cases:
        - Content Moderation
        - Video Summaries
        - Action Detection
        - Scene Understanding

        Args:
            video_bytes: Video als Bytes
            prompt: Frage/Anweisung zum Video
            model: Optional spezifisches Modell (gemini-2.0-pro empfohlen)
            context: Optional Kontext

        Returns:
            Dict mit analysis, key_frames, model_used
        """
        model_name = model or self.defaults["video"]
        model_wrapper = resolve_model(model_name)

        # Base64 encode video
        base64_video = base64.b64encode(video_bytes).decode("utf-8")

        result = await model_wrapper.run(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "video", "video": base64_video},
                    ],
                }
            ],
            context=context or {"mode": "video_analysis"},
        )

        return result

    # ---------------------------------------------------------
    # Embeddings: Text ‚Üí Vector (f√ºr RAG, Search, Memory)
    # ---------------------------------------------------------
    async def generate_embeddings(self, text: str, model: Optional[str] = None, context: Optional[dict] = None) -> dict:
        """
        Erstellt Embeddings f√ºr RAG, Search, Similarity.

        Use Cases:
        - Semantic Search
        - Document Retrieval
        - Memory System
        - Code Search
        - Similar Content Finding

        Args:
            text: Text zum Embedden
            model: Optional spezifisches Modell
            context: Optional Kontext

        Returns:
            Dict mit embedding_vector, model_used, dimensions
        """
        model_name = model or self.defaults["embeddings"]
        model_wrapper = resolve_model(model_name)

        result = await model_wrapper.run(
            messages=[{"role": "user", "content": text}],
            context=context or {"mode": "embeddings"},
        )

        return result

    # ---------------------------------------------------------
    # File Analysis: PDF, Code, JSON, Markdown ‚Üí Analyse
    # ---------------------------------------------------------
    async def analyze_file(
        self,
        file_bytes: bytes,
        filename: str,
        prompt: str = "Analysiere diese Datei",
        model: Optional[str] = None,
        context: Optional[dict] = None,
    ) -> dict:
        """
        Analysiert Datei-Inhalte.

        Use Cases:
        - PDF Text Extraction
        - Code Review
        - JSON Schema Analysis
        - Markdown ‚Üí Structured Data
        - Documentation Analysis

        Args:
            file_bytes: Datei als Bytes
            filename: Dateiname (wichtig f√ºr Type Detection)
            prompt: Frage/Anweisung zur Datei
            model: Optional spezifisches Modell
            context: Optional Kontext

        Returns:
            Dict mit analysis, extracted_data, model_used
        """
        model_name = model or self.defaults["vision"]
        model_wrapper = resolve_model(model_name)

        # Base64 encode file
        base64_file = base64.b64encode(file_bytes).decode("utf-8")

        result = await model_wrapper.run(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"{prompt}\n\nDatei: {filename}"},
                        {
                            "type": "file",
                            "file_data": base64_file,
                            "filename": filename,
                        },
                    ],
                }
            ],
            context=context or {"mode": "file_analysis"},
        )

        return result

    # ---------------------------------------------------------
    # Helper: Beste Modell-Auswahl f√ºr Task
    # ---------------------------------------------------------
    def pick_best_model_for_task(self, task_type: str, priority: str = "quality") -> str:
        """
        W√§hlt bestes Modell f√ºr Task-Type.

        Args:
            task_type: vision, audio, image_gen, video, embeddings
            priority: quality, speed, cost

        Returns:
            Modell-Name
        """
        if priority == "speed":
            task_models = {
                "vision": "gemini-2.0-flash",
                "audio": "whisper-1",
                "image_gen": "gpt-image-1",
                "video": "gemini-2.0-flash",
                "embeddings": "text-embedding-3-small",
            }
        elif priority == "cost":
            task_models = {
                "vision": "gemini-2.0-flash",
                "audio": "whisper-1",
                "image_gen": "gpt-image-1",
                "video": "gemini-2.0-flash",
                "embeddings": "text-embedding-3-small",
            }
        else:  # quality
            task_models = {
                "vision": "gpt-4o",
                "audio": "whisper-1",
                "image_gen": "gpt-image-1",
                "video": "gemini-2.0-pro",
                "embeddings": "text-embedding-3-large",
            }

        return task_models.get(task_type, self.defaults["vision"])


# Globale Instanz f√ºr Production Use
multimodal_ai_v2 = MultimodalAIV2()
